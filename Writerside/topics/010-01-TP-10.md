# TP 10 : Mise en Place de la Centralisation des Logs avec EFK

### Objectifs Pédagogiques

À la fin de ce TP, vous serez capable de :

* Ajouter Elasticsearch, Fluentd et Kibana à votre fichier `docker-compose.yml`.
* Créer un fichier de configuration pour Fluentd afin de collecter les logs Docker.
* Modifier la configuration Logback de vos microservices pour qu'ils produisent des logs au format JSON.
* Générer du trafic et utiliser Kibana pour visualiser, rechercher et filtrer les logs de manière centralisée.
* Identifier et utiliser l'ID de trace pour suivre une requête.

### Introduction : Assembler le centre de tri

La théorie est claire, passons à la construction. Nous allons ajouter trois nouveaux "bâtiments" à notre ville "
GestBook" via Docker Compose :

1. **L'entrepôt (Elasticsearch) :** pour stocker tous les logs.
2. **L'agence postale (Fluentd) :** pour collecter les logs de tous les autres conteneurs et les livrer à l'entrepôt.
3. **Le bureau de consultation (Kibana) :** pour nous permettre de fouiller dans l'entrepôt.

Ensuite, nous allons "former" nos employés (nos microservices) pour qu'ils écrivent leurs rapports (logs) dans un format
standardisé (JSON) que tout le monde peut comprendre. À la fin, une seule interface, Kibana, nous donnera une vue
complète sur l'activité de toute notre application.

### Étape 1 : Ajout de la Stack EFK à Docker Compose

<procedure>
<p>Ouvrez votre fichier <code>docker-compose.yml</code>. Nous allons y ajouter trois nouveaux services.</p>

```yaml
# Dans docker-compose.yml
services:
  # ... (tous vos services existants : config-server, eureka, etc.)

  # 7. Elasticsearch : La base de données de logs
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.13.0 # Utilisez une version récente
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      - discovery.type=single-node # Mode mono-noeud pour le développement
      - xpack.security.enabled=false # Désactive la sécurité pour simplifier
    volumes:
      - es_data:/usr/share/elasticsearch/data

  # 8. Kibana : L'interface de visualisation
  kibana:
    image: docker.elastic.co/kibana/kibana:8.13.0
    ports:
      - "5601:5601" # Port de l'interface web de Kibana
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200 # Dit à Kibana où trouver ES
    depends_on:
      - elasticsearch

  # 9. Fluentd : Le collecteur de logs
  fluentd:
    image: fluent/fluentd:v1.16-1
    volumes:
      # On monte notre futur fichier de configuration
      - ./fluentd/conf:/fluentd/etc
      # On monte le socket Docker pour que Fluentd puisse accéder aux logs des conteneurs
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - elasticsearch

# ...
volumes:
  # ... (postgres_data si vous l'avez)
  es_data: # Déclaration du volume pour les données d'Elasticsearch
```

<warning>
<b>Configuration d'Elasticsearch</b>
<p>
Les versions récentes d'Elasticsearch activent la sécurité par défaut (HTTPS, authentification). Pour un TP, il est beaucoup plus simple de la désactiver avec <code>xpack.security.enabled=false</code>. En production, il faudrait gérer les certificats et les mots de passe.
</p>
</warning>
</procedure>

### Étape 2 : Configuration de Fluentd

Fluentd a besoin d'un fichier de configuration pour savoir quoi faire.

<procedure>
<p>1. À la racine de votre projet <code>gestbook-microservices</code>, créez un nouveau dossier <code>fluentd</code>.</p>
<p>2. À l'intérieur de `fluentd`, créez un dossier <code>conf</code>.</p>
<p>3. Dans <code>fluentd/conf</code>, créez un fichier <code>fluent.conf</code>.</p>

```apache
# fluent.conf

# === INPUT ===
# D'où viennent les logs ?
# On utilise le plugin 'forward' pour écouter les logs envoyés
# par le logging driver de Docker.
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

# === FILTER ===
# Transformer les logs avant de les envoyer.
# On veut parser le message de log s'il est au format JSON.
<filter **>
  @type parser
  key_name log
  <parse>
    @type json
  </parse>
</filter>

# === OUTPUT ===
# Où envoyer les logs ?
# On utilise le plugin 'elasticsearch'.
<match **>
  @type elasticsearch
  host elasticsearch
  port 9200
  logstash_format true
  logstash_prefix fluentd
  logstash_dateformat %Y%m%d
  include_tag_key true
  type_name _doc
  tag_key @log_name
  # Gère les erreurs de connexion à Elasticsearch
  <buffer>
    @type file
    path /fluentd/log/buffer
    flush_interval 10s
  </buffer>
</match>
```

<p>4. Nous devons maintenant dire à Docker Compose que nos services doivent envoyer leurs logs à Fluentd. Modifiez chaque service applicatif (<code>product-service</code>, <code>order-service</code>, etc.) dans <code>docker-compose.yml</code> en ajoutant un bloc <code>logging</code>.</p>
<pre><code class="yaml">
# Exemple pour product-service (à faire pour tous les services applicatifs)
product-service:
  image: product-service:latest
  # ... (ports, environment, depends_on)
  logging:
    driver: "fluentd"
    options:
      fluentd-address: localhost:24224
      tag: gestbook.{{.Name}} # Tag les logs avec le nom du conteneur
</code></pre>
</procedure>

### Étape 3 : Configurer les Microservices pour Logger en JSON

Pour que Fluentd puisse bien parser les logs, nos applications Spring Boot doivent les écrire en JSON. Nous allons
utiliser `logstash-logback-encoder` pour cela.

<procedure>
<p><b>Pour chaque microservice</b> (`product-service`, `order-service`, `api-gateway`, etc.) :</p>
<p>1. <b>Ajouter la dépendance dans `pom.xml`</b></p>
```xml
<dependency>
    <groupId>net.logstash.logback</groupId>
    <artifactId>logstash-logback-encoder</artifactId>
    <version>7.4</version> <!-- Utilisez une version récente -->
</dependency>
```

<p>2. <b>Créer un fichier de configuration Logback</b></p>
<p>Dans <code>src/main/resources</code>, créez un fichier <code>logback-spring.xml</code>. Ce fichier va surcharger la configuration de logging par défaut de Spring Boot.</p>
```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <!-- Inclut la configuration de base de Spring Boot -->
    <include resource="org/springframework/boot/logging/logback/base.xml"/>

    <!-- Appender pour la console qui écrit en JSON -->
    <appender name="CONSOLE_JSON"
              class="ch.qos.logback.core.ConsoleAppender">
        <encoder
            class="net.logstash.logback.encoder.LogstashEncoder">
            <!-- Ajoute le nom du service aux logs -->
            <customFields>{"service_name":"${spring.application.name}"}</customFields>
        </encoder>
    </appender>

    <!-- On remplace l'appender par défaut par notre appender JSON -->
    <root level="INFO">
        <appender-ref ref="CONSOLE_JSON" />
    </root>

</configuration>
```

<p>3. <b>Ajouter la dépendance Micrometer Tracing</b></p>
<p>Pour avoir les `trace_id` et `span_id` dans nos logs, ajoutons Micrometer Tracing. Cela remplacera Spring Cloud Sleuth qui est maintenant déprécié.</p>
```xml
<!-- Dépendance pour l'instrumentation du tracing -->
<dependency>
    <groupId>io.micrometer</groupId>
    <artifactId>micrometer-tracing-bridge-brave</artifactId>
</dependency>
```

<p>4. <b>Reconstruire les images Docker</b></p>
<p>Après avoir fait ces modifications pour <b>tous</b> vos services, n'oubliez pas de reconstruire leurs images Docker (`mvn clean package` et `docker build ...`).</p>
</procedure>

### Étape 4 : Lancement et Exploration avec Kibana

<procedure>
<p>1. Lancez tout l'écosystème avec <code>docker-compose up</code>.</p>
<p>2. Attendez que tout soit démarré. Cela peut prendre un peu de temps, surtout pour Elasticsearch.</p>
<p>3. Ouvrez Kibana dans votre navigateur : <a href="http://localhost:5601/">http://localhost:5601/</a>.</p>
<p>4. <b>Configurer Kibana (première fois)</b></p>
<ul>
    <li>Cliquez sur le menu "hamburger" en haut à gauche, allez dans "Management" > "Stack Management".</li>
    <li>Sous "Kibana", cliquez sur "Data Views" (ou "Index Patterns").</li>
    <li>Cliquez sur "Create data view".</li>
    <li>Dans le champ "Name", mettez un nom, par exemple `gestbook-logs`.</li>
    <li>Dans le champ "Index pattern", tapez `fluentd*`. Kibana devrait détecter les index créés par Fluentd.</li>
    <li>Dans "Timestamp field", sélectionnez `@timestamp`.</li>
    <li>Cliquez sur "Save data view to Kibana".</li>
</ul>

<p>5. <b>Explorer les logs</b></p>
<ul>
    <li>Cliquez à nouveau sur le menu hamburger et allez dans "Analytics" > "Discover".</li>
    <li>Vous y êtes ! Vous devriez voir un flux de logs JSON en provenance de tous vos conteneurs.</li>
</ul>

<p>6. <b>Générer du trafic et filtrer</b></p>
<ul>
    <li>Utilisez votre client HTTP pour faire quelques appels à votre API via la Gateway (ex: <code>GET /api/products</code>, <code>POST /api/orders</code>).</li>
    <li>Rafraîchissez la vue "Discover" dans Kibana. De nouveaux logs apparaissent.</li>
    <li>Cliquez sur un log de l'<code>api-gateway</code>. Repérez le champ <code>trace.trace_id</code>. Cliquez sur la petite icône "+" à côté pour filtrer sur cette valeur.</li>
    <li>Magie ! Kibana n'affiche plus que les logs de tous les services qui correspondent à cette seule requête. Vous pouvez suivre son parcours de bout en bout.</li>
    <li>Utilisez la barre de recherche KQL (Kibana Query Language) en haut pour filtrer :
        <ul>
            <li><code>level : "ERROR"</code></li>
            <li><code>service_name : "product-service"</code></li>
            <li><code>message : *stock*</code></li>
        </ul>
    </li>
</ul>
<img src="img_4.png" alt="Vue Discover dans Kibana" />
</procedure>

---

### Exercice 13 : Logger des informations métier

**Contexte :** Les logs techniques c'est bien, mais les logs métier, c'est mieux. Nous voulons ajouter des champs
personnalisés à nos logs JSON pour faciliter la recherche.

**Votre mission :**

1. Dans le `ProductController` de `product-service`, modifiez la méthode `getProductById`.
2. Utilisez SLF4J et son MDC (Mapped Diagnostic Context) pour ajouter l'ID du produit demandé au contexte de log.
3. Faites en sorte que votre log de succès contienne cette information.
4. Faites un appel à `GET /api/products/1` via la Gateway.
5. Retrouvez le log correspondant dans Kibana et vérifiez qu'un nouveau champ contenant `productId=1` est apparu.

**Indice :** L'utilisation du MDC se fait avec `MDC.put("cle", "valeur");` avant le log, et `MDC.remove("cle");` après
pour nettoyer.

#### Correction exercice 13 {collapsible='true'}

1. et 2. Voici le `ProductController` modifié.

    ```java
    // Dans ProductController.java du projet product-service
    import org.slf4j.Logger;
    import org.slf4j.LoggerFactory;
    import org.slf4j.MDC; // Important !

    // ...

    @RestController
    @RequestMapping("/api/products")
    public class ProductController {
        private static final Logger LOGGER = LoggerFactory.getLogger(ProductController.class);
        // ... (repository)

        @GetMapping("/{id}")
        public ResponseEntity<Product> getProductById(@PathVariable Long id) {
            // Ajoute l'ID du produit au contexte de log pour cette requête
            MDC.put("productId", String.valueOf(id));

            LOGGER.info("Attempting to find product.");
            Optional<Product> product = productRepository.findById(id);

            // Nettoie le contexte pour ne pas polluer les logs suivants
            MDC.remove("productId");

            return product.map(ResponseEntity::ok)
                          .orElseGet(() -> ResponseEntity.notFound().build());
        }

        // ...
    }
    ```
   L'encodeur Logstash que nous avons configuré sait lire le MDC et ajouter automatiquement ses clés/valeurs comme
   champs dans le JSON final.

3. à 5. Après avoir reconstruit l'image, relancé l'écosystème et fait l'appel, vous trouverez le log dans Kibana. En
   l'inspectant, vous verrez un nouveau champ `productId: "1"` au même niveau que `message`, `level`, etc. Vous pouvez
   maintenant filtrer sur ce champ métier !

---

### Auto-évaluation

1. **(QCM)** Quelle est la fonction principale de Kibana dans la stack EFK ?
    * A) Stocker les logs.
    * B) Collecter les logs.
    * C) Visualiser et interroger les logs.
    * D) Formater les logs en JSON.
2. **(Question ouverte)** Pourquoi doit-on configurer le "logging driver" dans `docker-compose.yml` pour chaque
   service ?
3. **(QCM)** La dépendance `micrometer-tracing-bridge-brave` est principalement utilisée pour :
    * A) Formater les logs en JSON.
    * B) Envoyer les logs à Elasticsearch.
    * C) Générer et propager les ID de trace et de span.
    * D) Sécuriser les endpoints de logs.
4. **(Question ouverte)** J'ai un bug dans `order-service` qui ne se produit que lorsqu'il est appelé par l'
   `api-gateway`. Comment puis-je utiliser Kibana pour voir uniquement les logs de `order-service` qui ont été initiés
   par une requête spécifique à la gateway ?
5. **(QCM)** Dans la configuration `fluent.conf`, la section `<source>` définit :
    * A) Où les logs doivent être envoyés.
    * B) Comment les logs doivent être transformés.
    * C) D'où les logs proviennent.
    * D) Le format de l'index dans Elasticsearch.

---

### Conclusion

Vous avez transformé votre application "aveugle" en un système **observable**. La mise en place de la centralisation des
logs est un investissement qui porte ses fruits dès le premier incident en production.

Grâce à la stack **EFK** et à la puissance des **logs structurés**, vous êtes maintenant capable de :

* Consolider les logs de tous vos services en un seul endroit.
* Effectuer des recherches et des filtrages complexes via l'interface de **Kibana**.
* Suivre une requête de bout en bout grâce aux **ID de trace**.

Votre capacité à diagnostiquer et à déboguer votre architecture distribuée a fait un bond de géant. Il nous reste une
dernière pièce à ajouter au puzzle de l'observabilité : les métriques et le monitoring, pour passer du "que s'est-il
passé ?" (logs) au "comment ça va en ce moment ?" (monitoring). Ce sera l'objet du prochain module avec Prometheus et
Grafana.